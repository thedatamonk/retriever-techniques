{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24664d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "500e840c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"Pad Thai is a popular Thai stir-fried noodle dish with tamarind sauce, peanuts, and lime\",\n",
    "    \"Sushi is a Japanese dish featuring vinegared rice combined with raw fish and vegetables\",\n",
    "    \"Biryani is a fragrant Indian rice dish cooked with aromatic spices, meat, and saffron\",\n",
    "    \"Pho is a Vietnamese soup with rice noodles, herbs, and beef or chicken broth\",\n",
    "    \"Ramen consists of Chinese-style wheat noodles served in a flavorful meat or fish broth\",\n",
    "    \"Dim sum includes various Chinese small bite-sized portions served in steamer baskets\",\n",
    "    \"Kimchi is a traditional Korean fermented vegetable dish made primarily with napa cabbage\",\n",
    "    \"Tom Yum is a hot and sour Thai soup with lemongrass, lime leaves, and shrimp\",\n",
    "    \"Dumplings are filled dough pockets found across Asian cuisines with meat or vegetable fillings\",\n",
    "    \"Butter chicken is a creamy Indian curry dish with tender chicken in tomato-based sauce\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d50159b",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"Thai soup spicy\",\n",
    "    \"noodles broth Asian\",\n",
    "    \"Indian rice spices\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61685bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_matrix(matrix, precision=2):\n",
    "    \"\"\"Pretty print a 2D numpy array\"\"\"\n",
    "    rows, cols = matrix.shape\n",
    "    \n",
    "    # Find max width needed for formatting\n",
    "    max_width = max(len(f\"{val:.{precision}f}\") for row in matrix for val in row)\n",
    "    \n",
    "    for i in range(rows):\n",
    "        row_str = \" \".join(f\"{matrix[i, j]:>{max_width}.{precision}f}\" for j in range(cols))\n",
    "        print(row_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fbd46e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, tokenized_docs) -> None:\n",
    "        self.vocab = self.compute_vocab(tokenized_docs)\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.token2id = {token: idx for idx, token in enumerate(self.vocab)}\n",
    "\n",
    "    def compute_vocab(self, tokenized_docs):\n",
    "        vocab = set()\n",
    "        for token_doc in tokenized_docs:\n",
    "            vocab.update(token_doc)\n",
    "        \n",
    "        vocab = sorted(vocab)\n",
    "        print(f\"Vocab created of size: {len(vocab)}\")\n",
    "        return vocab\n",
    "\n",
    "UNKNOWN_TOKEN_ID = -1\n",
    "\n",
    "class BM25:\n",
    "    def __init__(self, docs, k1=1.2, b=0.75) -> None:\n",
    "        self.docs = [self.tokenize(doc) for doc in docs]\n",
    "        self.doc_lengths = [len(doc) for doc in self.docs]\n",
    "        self.avg_doclen = np.mean(self.doc_lengths)\n",
    "        self.vocab = Vocab(tokenized_docs=self.docs)\n",
    "        self.n_docs = len(self.docs)\n",
    "        self.dxv_matrix = self._compute_doc_x_vocab_matrix()\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        # Lowercase and split on non-alphanumeric characters\n",
    "        tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        return tokens\n",
    "    \n",
    "    def _compute_doc_x_vocab_matrix(self):\n",
    "        dxv_matrix = np.zeros(shape=(self.n_docs, self.vocab.vocab_size))\n",
    "        # Iterate over all the docs\n",
    "        for doc_id in range(self.n_docs):\n",
    "            for token in self.docs[doc_id]:\n",
    "                token_id = self.vocab.token2id[token]\n",
    "                dxv_matrix[doc_id][token_id] += 1 # increase occurance count of token with id `token_id` in doc of id `doc_id`\n",
    "        return dxv_matrix\n",
    "        \n",
    "    def compute_idf_term(self, token):\n",
    "        # compute n_q -> # of documents that contains the query term q\n",
    "        # self.n_docs\n",
    "        try:\n",
    "            token_id = self.vocab.token2id[token]\n",
    "        except KeyError:\n",
    "            token_id = UNKNOWN_TOKEN_ID\n",
    "        \n",
    "        if token_id != UNKNOWN_TOKEN_ID:\n",
    "            n_q = np.sum(self.dxv_matrix[:, token_id] != 0)\n",
    "            print(f\"`{token}` appeared in {n_q}/{self.n_docs} docs\")\n",
    "            return np.log((self.n_docs-n_q+0.5)/(n_q+0.5)+1)\n",
    "        else:\n",
    "            # TODO: Need to rethink what to return here. for now I will just take a guess and return -1\n",
    "            print(f\"`{token}` not found in vocab.\")\n",
    "            return -1\n",
    "\n",
    "    def compute_tf_term(self, token, doc_id):\n",
    "        token_id = self.vocab.token2id[token]\n",
    "\n",
    "        f_qd = self.dxv_matrix[doc_id][token_id]\n",
    "\n",
    "        return f_qd\n",
    "    \n",
    "    def compute_doc_norm(self, doc_id):\n",
    "        return (1 - self.b + (self.b * self.doc_lengths[doc_id]/self.avg_doclen))\n",
    "    \n",
    "    def compute_bm25(self, query: str):\n",
    "        query_tokens = self.tokenize(query)\n",
    "        score_final = np.zeros(self.n_docs)\n",
    "        idf_out = np.zeros(len(query_tokens))\n",
    "        for idx, token in enumerate(query_tokens):\n",
    "            idf_out[idx] = self.compute_idf_term(token=token)\n",
    "            \n",
    "        for doc_id in range(self.n_docs):\n",
    "            doc_norm = self.compute_doc_norm(doc_id=doc_id)\n",
    "            score_d = 0.0\n",
    "            for idx, token in enumerate(query_tokens):\n",
    "                f_qd = self.compute_tf_term(token=token, doc_id=doc_id)\n",
    "                score_dxt = idf_out[idx] * (f_qd*(self.k1+1)) / (f_qd + (self.k1 * doc_norm))\n",
    "                score_d += score_dxt\n",
    "            score_final[doc_id] = score_d\n",
    "\n",
    "        results = []\n",
    "        for doc_id in range(self.n_docs):\n",
    "            res = {\n",
    "                \"doc\": self.docs[doc_id],\n",
    "                \"bm25_score\": score_final[doc_id]\n",
    "            }\n",
    "            results.append(res)\n",
    "        \n",
    "        sorted_results = sorted(results, key=lambda x: x[\"bm25_score\"], reverse=True)\n",
    "        return sorted_results, score_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231dd327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab created of size: 92\n"
     ]
    }
   ],
   "source": [
    "bm25 = BM25(docs=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fc7a969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`thai` appeared in 2/10 docs\n",
      "`soup` appeared in 2/10 docs\n",
      "`chicken` appeared in 2/10 docs\n"
     ]
    }
   ],
   "source": [
    "query = \"thai soup chicken\"\n",
    "scores_with_docs, scores_only = bm25.compute_bm25(query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc7bf028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.97129536 0.         0.         2.98886046 0.         0.\n",
      " 0.         2.90503452 0.         2.00953994]\n"
     ]
    }
   ],
   "source": [
    "print(scores_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cace9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    # Lowercase and split on non-alphanumeric characters\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    return tokens\n",
    "\n",
    "tokenized_corpus = [tokenize(text=doc) for doc in docs]\n",
    "\n",
    "bm25_kapi = BM25Okapi(tokenized_corpus, k1=1.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be11c749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thai soup chicken\n"
     ]
    }
   ],
   "source": [
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8aa9e07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_query = tokenize(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "515f042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_scores = bm25_kapi.get_scores(tokenized_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff5a5c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.62825016 0.         0.         2.46873838 0.         0.\n",
      " 0.         2.39949985 0.         1.65983941]\n"
     ]
    }
   ],
   "source": [
    "print(doc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb5f155",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval (3.10.19)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
